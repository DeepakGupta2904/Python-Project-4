{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce170b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CodSoft Internship Program for Data Science (1 October 2023 to 31b October 2023) #####\n",
    "##### Name:- Deepak Gupta #####\n",
    "##### Task- 1 Titanic Survival Prediction #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cbbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1. Importing the packages and libraries:-\n",
    "##### let's start off by importing the necessary libraries for data analysis and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets load the required packages and libraries for data analysis\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## For data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 2. Reading and exploring the data:-\n",
    "##### let's read the training and testing datasets from the provided CSV files and use the .head() and .info() methods to take a\n",
    "##### glimpse at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a151ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the training and test datasets\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "\n",
    "test_df = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets take a look at our training data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now the test dataset\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82315b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets see what kind of data we have to work with\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3. Data analysis:-\n",
    "##### Now let's see what features we have to train our model on and what useful insights we can obtain from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42907bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing out a list of all the columns in our training dataset\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bc078",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Types of features:-\n",
    "\n",
    "##### Categorical : Pclass, Sex, Embarked, Survived\n",
    "##### Continuous : Age, Fare, Sibsp, Parch, PassengerId\n",
    "##### Alphanumeric: Ticket, Cabin, Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now that we know what kind of features we are going to work with, let's take a look what information they provide us:-\n",
    "## printing summary statistics\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d693313",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Observations from above summary statistics:-\n",
    "\n",
    "##### There are a total of 891 passengers in our training dataset.\n",
    "##### Since the Survived column has dicrete data, the mean gives us the number of people survived from 891 i.e. 38%.\n",
    "##### Most people belonged to Pclass = 3\n",
    "##### The maximum Fare paid for a ticket was 512 however the fare prices varied a lot as we can see from the standard deviation\n",
    "##### of 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8bad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7be5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Taking a look at our categorical features we find that:-\n",
    "\n",
    "##### The passneger column has two sexes with male being the most common.\n",
    "##### Cabin feature has many duplicate values.\n",
    "##### Embarked has three possible values with most passengers embarking from Southhampton.\n",
    "##### Names of all passengers are unique.\n",
    "##### Ticket column also has a fair amount of duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce647b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding the percantage of missing values in train dataset\n",
    "train_df.isnull().sum()/ len(train_df) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding the percentage of Null values in test dataset\n",
    "test_df.isnull().sum()/ len(test_df) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015eb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### As we can see the Age column and Embarked column are missing values that we will need to fill. The Cabin coulmn has 77% \n",
    "##### and 78% missing values in train and test datasets respectively hence, it might be worth considering dropping that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5811e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 4. Visual Data Analysis:-\n",
    "##### It's time to visualize our data and try to draw some inferences from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Sex feature:- let's begin by exploring the Sex column in our trainig data set.\n",
    "sns.countplot('Sex',data=train_df)\n",
    "train_df['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd79ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The number of males on board were clearly more than the female. Now let's see how their survival percentages were:-\n",
    "\n",
    "## Comparing the Sex feature against Survived\n",
    "sns.barplot(x='Sex',y='Survived',data=train_df)\n",
    "train_df.groupby('Sex',as_index=False).Survived.mean()\n",
    "\n",
    "## As one would assume the number of female who survived was much more than the males who survived i.e. 74% females as against \n",
    "## to 18% males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How did the Class of each passenger affect their survival?\n",
    "\n",
    "## Comparing the Pclass feature against Survived\n",
    "sns.barplot(x='Pclass',y='Survived',data=train_df)\n",
    "train_df[[\"Pclass\", \"Survived\"]].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
    "\n",
    "## Clearly Class had an effect on survival of each passenger with the percentages of survival being 62.96%, 47.28%, 24.23% for \n",
    "## Pclass 1, 2 and 3 respectively. Thus, belonging to Pclass = 1 had a huge advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91381f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Did the port from which the passengers embarked have an effect on their Survival?\n",
    "\n",
    "## Comparing the Embarked feature against Survived\n",
    "sns.barplot(x='Embarked',y='Survived',data=train_df)\n",
    "train_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff96259",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It seems that the passengers that embarked from port Cherbourg had a higher rate of Survival at 55%. This could be either due\n",
    "## to their Sex or socio-economic class. Let's move forward to see the effect of having parents or children on-board.\n",
    "\n",
    "sns.barplot(x='Parch',y='Survived',data=train_df)\n",
    "train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed47a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looks like passengers who had either 1, 2 or 3 had a higher possibility of surviving than the ones had none. However having\n",
    "## more than 3 made the possibility even lesser. Moving on to the effect of having spouse or siblings on Survival:\n",
    "\n",
    "sns.barplot(x='SibSp',y='Survived',data=train_df)\n",
    "train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
    "\n",
    "## It seems that having a spouse or 1 sibling had a positive effect on Survival as compared to being alone. Though the chances \n",
    "## of survival go down with the number of siblings after 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Age column has some missing values. We will take care of that later when we clean our training data. First we shall \n",
    "## proceed by:-\n",
    "## 1. Plotting a histogram of the age values .\n",
    "## 2. Taking a look at the median value of age as well as the spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae787dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Age.hist(bins=10,color='teal')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "print(\"The Median age of passengers is :\", int(train_df.Age.median()))\n",
    "print(\"The Standard Deviation age of passengers is :\", int(train_df.Age.std()))\n",
    "\n",
    "## It is obvious to assume that younger individuals were more likely to survive, however we should test our assumption before we\n",
    "## proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='Age',y='Survived',data=train_df,palette='Set1')\n",
    "\n",
    "## Our assumption was right, younger individuals were more likely to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From the sex column we saw that there was a significant difference in the percentage of men and women that survived. Does sex\n",
    "## also play a role when it comes to surviving the disaster along with the age?\n",
    "\n",
    "sns.lmplot(x='Age',y='Survived',data=train_df,hue='Sex',palette='Set1')\n",
    "\n",
    "## Interestingly, age has an opposite effect on the survival in men and women. The chances of survival increase as the age of \n",
    "## women increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22fe856",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takeaway: Age feature can have a different effect on the outcome depending on the sex of the passenger. Perhaps we can use \n",
    "## this information in feature engineering.\n",
    "\n",
    "## Checking for outliers in Age data\n",
    "sns.boxplot(x='Sex',y='Age',data=train_df)\n",
    "\n",
    "## getting the median age according to Sex\n",
    "train_df.groupby('Sex',as_index=False)['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting the Fare column to see the spread of data\n",
    "sns.boxplot(\"Fare\",data=train_df)\n",
    "\n",
    "## Checking the mean and median values\n",
    "print(\"Mean value of Fare is :\",train_df.Fare.mean())\n",
    "print(\"Median value of Fare is :\",train_df.Fare.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 5. Cleaning Data:-\n",
    "##### Now that we have visualized our data , we can proceed to fill in the NaN values in our test and train datasets and drop \n",
    "##### the columns that we will not require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef65caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's start off by dropping the coulmns we will not be needing\n",
    "drop_list=['Cabin','Ticket','PassengerId']\n",
    "\n",
    "train_df = train_df.drop(drop_list,axis=1)\n",
    "test_passenger_df = pd.DataFrame(test_df.PassengerId)\n",
    "test_df = test_df.drop(drop_list,axis=1)\n",
    "\n",
    "test_passenger_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, let's fill in the missing values for Embarked column in the training dataset. Most people embarked on their journey from\n",
    "## Southhampton port. Hence, we will be filling the two missing values with \"S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2741830",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filling the missing Embarked values in train and test datasets\n",
    "train_df.Embarked.fillna('S',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38671cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will replace the NaN values in the age column with the median age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filling the missing values in the Age column\n",
    "train_df.Age.fillna(28, inplace=True)\n",
    "test_df.Age.fillna(28, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8846e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There is a small fraction of fare values missing in the fare column which we will fill using the median value since there a \n",
    "## plenty of outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5782f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filling the null Fare values in test dataset\n",
    "test_df.Fare.fillna(test_df.Fare.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd16177",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6. Feature Engineering:-\n",
    "##### Title Feature The name column might not be useful to us directly but a lot of names have titles like Mr, Mrs, Lady, etc \n",
    "##### which might indicate the individual's status in the society which can affect the chance of survival.\n",
    "\n",
    "##### We shall try to extract a Title feature form the name column which might improve the performanc of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining train and test dataframes to work with them simultaneously\n",
    "Combined_data = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d136ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting the various title in Names column\n",
    "for dataset in Combined_data:\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "#Plotting the various titles extracted from the names    \n",
    "sns.countplot(y='Title',data=train_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f93707",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are some titles that are very rare like Capt and Lady. It would be better to group such titles under one name know as \n",
    "## 'rare'. Some titles also seem to be incorrectly spelled. They also need to be rectified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Refining the title feature by merging some titles\n",
    "for dataset in Combined_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Special')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace({'Mlle':'Miss','Ms':'Miss','Mme':'Mrs'})\n",
    "    \n",
    "train_df.groupby('Title',as_index=False)['Survived'].mean().sort_values(by='Survived',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now lets see the distribution of the title feature\n",
    "sns.countplot(y='Title',data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping the title names to numeric values\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Special\": 5}\n",
    "for dataset in Combined_data:\n",
    "    dataset['Title'] = dataset.Title.map(title_mapping)\n",
    "    dataset['Title'] = dataset.Title.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027657a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## As we observed from our data visualization being alone on the titanic had a disadvantage when it came to survival: Next we\n",
    "## will create a feature IsAlone which depends on the number of family members that can be calculated from the Parch and SibSp \n",
    "## columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e795349",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a new feature IsAlone from the SibSp and Parch columns\n",
    "for dataset in Combined_data:\n",
    "    dataset[\"Family\"] = dataset['SibSp'] + dataset['Parch']\n",
    "    dataset[\"IsAlone\"] = np.where(dataset[\"Family\"] > 0, 0,1)\n",
    "    dataset.drop('Family',axis=1,inplace=True)\n",
    "train_df.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92647f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting rid of the columns that are not required anymore:\n",
    "\n",
    "## dropping the Name,SibSP and Parch columns\n",
    "for dataset in Combined_data:\n",
    "    dataset.drop(['SibSp','Parch','Name'],axis=1,inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269264f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Age had big role to play when it came to survival. Clearly younger people were more likely to survive. Hence, it should be\n",
    "## worth considering a feature IsMinor for the passengers under the age of 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b258274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating another feature if the passenger is a child\n",
    "for dataset in Combined_data:\n",
    "    dataset[\"IsMinor\"] = np.where(dataset[\"Age\"] < 15, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Older female passengers also had a higher chance of survival. Let's create a feature name Old_female that would account for \n",
    "## women older tha 50 years on board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be53dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Old_Female'] = (train_df['Age']>50)&(train_df['Sex']=='female')\n",
    "train_df['Old_Female'] = train_df['Old_Female'].astype(int)\n",
    "\n",
    "test_df['Old_Female'] = (test_df['Age']>50)&(test_df['Sex']=='female')\n",
    "test_df['Old_Female'] = test_df['Old_Female'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pclass, Sex and Embarked are the categorical features in our data. we can convert these categorucal variables into dummy \n",
    "## variables using the get_dummies method in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting categorical variables into numerical ones\n",
    "train_df2 = pd.get_dummies(train_df,columns=['Pclass','Sex','Embarked'],drop_first=True)\n",
    "test_df2 = pd.get_dummies(test_df,columns=['Pclass','Sex','Embarked'],drop_first=True)\n",
    "train_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc84d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Age and Fare columns have continuous data and there might be fluctuations that do not reflect patterns in the data, which \n",
    "## might be noise. That's why wel put people that are within a certain range of age or fare in the same bin. This can be \n",
    "## achieved using qcut method in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd6374",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating Age bands\n",
    "train_df2['AgeBands'] = pd.qcut(train_df2.Age,4,labels=False) \n",
    "test_df2['AgeBands'] = pd.qcut(test_df2.Age,4,labels=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b91ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating Fare bands\n",
    "train_df2['FareBand'] = pd.qcut(train_df2.Fare,7,labels=False)\n",
    "test_df2['FareBand'] = pd.qcut(test_df2.Fare,7,labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e83cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the Age and Fare columns\n",
    "train_df2.drop(['Age','Fare'],axis=1,inplace=True)\n",
    "test_df2.drop(['Age','Fare'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae89b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's take a final look at our training and testing data before we proceed to build our model.\n",
    "train_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c270d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 7. Machine Learning:-\n",
    "##### We will try out some different ML models to see which gives us the best result. the process will be as follows:\n",
    "\n",
    "##### A. Importing the required machine learning libraries from scikit learn.\n",
    "##### B. Splitting out training data into train and test datasets to check the performance of our model.\n",
    "##### C. Try out different classifying model to see which fits the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb40cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the required ML libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting out training data into X: features and y: target\n",
    "X = train_df2.drop(\"Survived\",axis=1) \n",
    "y = train_df2[\"Survived\"]\n",
    "\n",
    "## splitting our training data again in train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df224cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "acc_logreg = round(accuracy_score(y_pred, y_test) * 100, 2)\n",
    "acc_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d115f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our score also depends on how we had split our training data using train_test_split. We should also perform k-fold cross \n",
    "## validation to get a more accurate score. Here we will be going with 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9cc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's perform some K-fold cross validation for logistic Regression\n",
    "cv_scores = cross_val_score(logreg,X,y,cv=5)\n",
    " \n",
    "np.mean(cv_scores)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "decisiontree = DecisionTreeClassifier()\n",
    "dep = np.arange(1,10)\n",
    "param_grid = {'max_depth' : dep}\n",
    "\n",
    "clf_cv = GridSearchCV(decisiontree, param_grid=param_grid, cv=5)\n",
    "\n",
    "clf_cv.fit(X, y)\n",
    "clf_cv.best_params_,clf_cv.best_score_*100\n",
    "print('Best value of max_depth:',clf_cv.best_params_)\n",
    "print('Best score:',clf_cv.best_score_*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84984c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest CLassifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "ne = np.arange(1,20)\n",
    "param_grid = {'n_estimators' : ne}\n",
    "\n",
    "rf_cv = GridSearchCV(random_forest, param_grid=param_grid, cv=5)\n",
    "\n",
    "rf_cv.fit(X, y)\n",
    "print('Best value of n_estimators:',rf_cv.best_params_)\n",
    "print('Best score:',rf_cv.best_score_*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386aefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbk = GradientBoostingClassifier()\n",
    "ne = np.arange(1,20)\n",
    "dep = np.arange(1,10)\n",
    "param_grid = {'n_estimators' : ne,'max_depth' : dep}\n",
    "\n",
    "gbk_cv = GridSearchCV(gbk, param_grid=param_grid, cv=5)\n",
    "\n",
    "gbk_cv.fit(X, y)\n",
    "print('Best value of parameters:',gbk_cv.best_params_)\n",
    "print('Best score:',gbk_cv.best_score_*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022fd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0dd4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963715a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
